{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "N Demo\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing the required libraries and starting the PyTorch directML device."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:20:10.546174Z",
     "end_time": "2023-04-20T20:20:11.212182Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_directml\n",
    "import torch.nn as nn\n",
    "dml = torch_directml.device()\n",
    "print(dml)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "Let's start by creating some sample data using the `torch.tensor` command. In Numpy, this could be done with `np.array`. Both functions serve the same purpose, but in PyTorch everything is a Tensor as opposed to a vector or matrix. We define types in PyTorch using the `dtype=torch.xxx` command.\n",
    "\n",
    "In the data below, `X` represents the amount of hours studied and how much time students spent sleeping, whereas `y` represent grades. The variable `xPredicted` is a single input for which we want to predict a grade using the parameters learned by the neural network. Remember, the neural network wants to learn a mapping between `X` and `y`, so it will try to take a guess from what it has learned from the training data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float).to(dml) # 3 X 2 tensor\n",
    "y = torch.tensor(([92], [100], [89]), dtype=torch.float).to(dml) # 3 X 1 tensor\n",
    "xPredicted = torch.tensor(([4, 8]), dtype=torch.float).to(dml) # 1 X 2 tensor\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:20:11.215247Z",
     "end_time": "2023-04-20T20:20:11.320525Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can check the size of the tensors we have just created with the size command. This is equivalent to the shape command used in tools such as Numpy and Tensorflow."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 1])\n",
      "privateuseone:0\n",
      "privateuseone:0\n"
     ]
    }
   ],
   "source": [
    "print(X.size())\n",
    "print(y.size())\n",
    "\n",
    "#Check if the tensors are on the GPU direct ML device\n",
    "print(X.device)\n",
    "print(y.device)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:20:11.322435Z",
     "end_time": "2023-04-20T20:20:11.324800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Scaling\n",
    "Below we are performing some scaling on the sample data. Notice that the max function returns both a tensor and the corresponding indices. So we use _ to capture the indices which we won't use here because we are only interested in the max values to conduct the scaling. Perfect! Our data is now in a very nice format our neural network will appreciate later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 1.0000], device='privateuseone:0')\n",
      "tensor([[0.9200],\n",
      "        [1.0000],\n",
      "        [0.8900]], device='privateuseone:0')\n"
     ]
    }
   ],
   "source": [
    "# scale units\n",
    "X_max, _ = torch.max(X, 0)\n",
    "xPredicted_max, _ = torch.max(xPredicted, 0)\n",
    "\n",
    "X = torch.div(X, X_max)\n",
    "xPredicted = torch.div(xPredicted, xPredicted_max)\n",
    "y = y / 100  # max test score is 100\n",
    "print(xPredicted)\n",
    "print(y)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:20:11.327376Z",
     "end_time": "2023-04-20T20:20:11.360445Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model (Computation Graph)\n",
    "Once the data has been processed and it is in the proper format, all you need to do now is to define your model. Here is where things begin to change a little as compared to how you would build your neural networks using, say, something like Keras or Tensorflow. However, you will realize quickly as you go along that PyTorch doesn't differ much from other deep learning tools. At the end of the day we are constructing a computation graph, which is used to dictate how data should flow and what type of operations are performed on this information.\n",
    "\n",
    "For illustration purposes, we are building the following neural network or computation graph:\n",
    "\n",
    "![alt text](https://drive.google.com/uc?export=view&id=1l-sKpcCJCEUJV1BlAqcVAvLXLpYCInV6)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        # parameters\n",
    "        # TODO: parameters can be parameterized instead of declaring them here\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "\n",
    "        # weights\n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenSize).to(dml) # 3 X 2 tensor\n",
    "        self.W2 = torch.randn(self.hiddenSize, self.outputSize).to(dml) # 3 X 1 tensor\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z = torch.matmul(X, self.W1) # 3 X 3 \".dot\" does not broadcast in PyTorch\n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        # derivative of sigmoid\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o # error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error\n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X).to(dml), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2).to(dml), self.o_delta)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        # forward + backward pass for training\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "\n",
    "    def saveWeights(self, model):\n",
    "        # we will use the PyTorch internal storage functions\n",
    "        torch.save(model, \"NN\")\n",
    "        # you can reload model with all the weights and so forth with:\n",
    "        # torch.load(\"NN\")\n",
    "\n",
    "    def predict(self):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(xPredicted))\n",
    "        print (\"Output: \\n\" + str(self.forward(xPredicted)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:20:11.365538Z",
     "end_time": "2023-04-20T20:20:11.406440Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For the purpose of this tutorial, we are not going to be talking math stuff, that's for another day. I just want you to get a gist of what it takes to build a neural network from scratch using PyTorch. Let's break down the model which was declared via the class above.\n",
    "\n",
    "Class Header\n",
    "First, we defined our model via a class because that is the recommended way to build the computation graph. The class header contains the name of the class Neural Network and the parameter nn.Module which basically indicates that we are defining our own neural network."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:20:11.406260Z",
     "end_time": "2023-04-20T20:20:11.406683Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "nitialization\n",
    "The next step is to define the initializations ( def __init__(self,)) that will be performed upon creating an instance of the customized neural network. You can declare the parameters of your model here, but typically, you would declare the structure of your network in this section -- the size of the hidden layers and so forth. Since we are building the neural network from scratch, we explicitly declared the size of the weights matrices: one that stores the parameters from the input to hidden layer; and one that stores the parameter from the hidden to output layer. Both weight matrices are initialized with values randomly chosen from a normal distribution via torch.randn(...). Note that we are not using bias just to keep things as simple as possible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:20:11.406303Z",
     "end_time": "2023-04-20T20:20:11.406755Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Forward Function\n",
    "The forward function is where all the magic happens (see below). This is where the data enters and is fed into the computation graph (i.e., the neural network structure we have built). Since we are building a simple neural network with one hidden layer, our forward function looks very simple:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:20:11.406325Z",
     "end_time": "2023-04-20T20:20:11.406837Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The forward function above takes the input Xand then performs a matrix multiplication (torch.matmul(...)) with the first weight matrix self.W1. Then the result is applied an activation function, sigmoid. The resulting matrix of the activation is then multiplied with the second weight matrix self.W2. Then another activation if performed, which renders the output of the neural network or computation graph. The process I described above is simply what's known as a feedforward pass. In order for the weights to optimize when training, we need a backpropagation algorithm.\n",
    "\n",
    "The Backward Function\n",
    "The backward function contains the backpropagation algorithm, where the goal is to essentially minimize the loss with respect to our weights. In other words, the weights need to be updated in such a way that the loss decreases while the neural network is training (well, that is what we hope for). All this magic is possible with the gradient descent algorithm which is declared in the backward function. Take a minute or two to inspect what is happening in the code below:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:20:11.406354Z",
     "end_time": "2023-04-20T20:20:11.406899Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that we are performing a lot of matrix multiplications along with the transpose operations via the torch.matmul(...) and torch.t(...) operations, respectively. The rest is simply gradient descent -- there is nothing to it.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training\n",
    "All that is left now is to train the neural network. First we create an instance of the computation graph we have just built:\n",
    "\n",
    "NN = Neural_Network()\n",
    "Then we train the model for 1000 rounds. Notice that in PyTorch NN(X) automatically calls the forward function so there is no need to explicitly call NN.forward(X).\n",
    "\n",
    "After we have obtained the predicted output for ever round of training, we compute the loss, with the following code:\n",
    "\n",
    "torch.mean((y - NN(X))**2).detach().item()\n",
    "The next step is to start the training (foward + backward) via NN.train(X, y). After we have trained the neural network, we can store the model and output the predicted value of the single instance we declared in the beginning, xPredicted.\n",
    "\n",
    "Let's train!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "#0 Loss: 0.5542240738868713\n",
      "#100 Loss: 0.0033749414142221212\n",
      "#200 Loss: 0.003027650760486722\n",
      "#300 Loss: 0.002850624965503812\n",
      "#400 Loss: 0.0026696377899497747\n",
      "#500 Loss: 0.002477443078532815\n",
      "#600 Loss: 0.0022787281777709723\n",
      "#700 Loss: 0.002080590231344104\n",
      "#800 Loss: 0.0018902977462857962\n",
      "#900 Loss: 0.0017136790556833148\n",
      "#1000 Loss: 0.0015542961191385984\n",
      "#1100 Loss: 0.0014134763041511178\n",
      "#1200 Loss: 0.0012908378848806024\n",
      "#1300 Loss: 0.001184956287033856\n",
      "#1400 Loss: 0.0010939311468973756\n",
      "#1500 Loss: 0.0010157356737181544\n",
      "#1600 Loss: 0.0009484269539825618\n",
      "#1700 Loss: 0.0008902717381715775\n",
      "#1800 Loss: 0.0008397704223170877\n",
      "#1900 Loss: 0.0007956382469274104\n",
      "#2000 Loss: 0.000756812107283622\n",
      "#2100 Loss: 0.0007224125438369811\n",
      "#2200 Loss: 0.0006917092832736671\n",
      "#2300 Loss: 0.0006641148356720805\n",
      "#2400 Loss: 0.0006391342612914741\n",
      "#2500 Loss: 0.0006163753569126129\n",
      "#2600 Loss: 0.0005955039523541927\n",
      "#2700 Loss: 0.0005762592772953212\n",
      "#2800 Loss: 0.0005584147875197232\n",
      "#2900 Loss: 0.0005417938809841871\n",
      "#3000 Loss: 0.000526241201441735\n",
      "#3100 Loss: 0.0005116363172419369\n",
      "#3200 Loss: 0.0004978737561032176\n",
      "#3300 Loss: 0.0004848645767197013\n",
      "#3400 Loss: 0.0004725402395706624\n",
      "#3500 Loss: 0.0004608349408954382\n",
      "#3600 Loss: 0.00044969195732846856\n",
      "#3700 Loss: 0.0004390735412016511\n",
      "#3800 Loss: 0.00042893103091046214\n",
      "#3900 Loss: 0.00041923936805687845\n",
      "#4000 Loss: 0.0004099629004485905\n",
      "#4100 Loss: 0.00040107526001520455\n",
      "#4200 Loss: 0.0003925494966097176\n",
      "#4300 Loss: 0.00038436584873124957\n",
      "#4400 Loss: 0.0003765079309232533\n",
      "#4500 Loss: 0.00036895423545502126\n",
      "#4600 Loss: 0.00036168581573292613\n",
      "#4700 Loss: 0.00035469423164613545\n",
      "#4800 Loss: 0.00034795867395587265\n",
      "#4900 Loss: 0.00034146764664910734\n",
      "#5000 Loss: 0.00033521122531965375\n",
      "#5100 Loss: 0.0003291772154625505\n",
      "#5200 Loss: 0.00032335423748008907\n",
      "#5300 Loss: 0.0003177305043209344\n",
      "#5400 Loss: 0.00031229876913130283\n",
      "#5500 Loss: 0.00030704945675097406\n",
      "#5600 Loss: 0.0003019733994733542\n",
      "#5700 Loss: 0.00029706390341743827\n",
      "#5800 Loss: 0.00029231226653791964\n",
      "#5900 Loss: 0.0002877104270737618\n",
      "#6000 Loss: 0.00028325399034656584\n",
      "#6100 Loss: 0.0002789356803987175\n",
      "#6200 Loss: 0.00027474871603772044\n",
      "#6300 Loss: 0.0002706891391426325\n",
      "#6400 Loss: 0.00026675002300180495\n",
      "#6500 Loss: 0.0002629256050568074\n",
      "#6600 Loss: 0.0002592118689790368\n",
      "#6700 Loss: 0.00025560540962032974\n",
      "#6800 Loss: 0.00025209999876096845\n",
      "#6900 Loss: 0.00024869051412679255\n",
      "#7000 Loss: 0.0002453784109093249\n",
      "#7100 Loss: 0.0002421543758828193\n",
      "#7200 Loss: 0.00023901608074083924\n",
      "#7300 Loss: 0.00023596126993652433\n",
      "#7400 Loss: 0.00023298639280255884\n",
      "#7500 Loss: 0.0002300873602507636\n",
      "#7600 Loss: 0.00022726341558154672\n",
      "#7700 Loss: 0.00022451052791438997\n",
      "#7800 Loss: 0.00022182594693731517\n",
      "#7900 Loss: 0.00021920682047493756\n",
      "#8000 Loss: 0.0002166520425816998\n",
      "#8100 Loss: 0.00021415860101114959\n",
      "#8200 Loss: 0.00021172342530917376\n",
      "#8300 Loss: 0.00020934591884724796\n",
      "#8400 Loss: 0.00020702258916571736\n",
      "#8500 Loss: 0.0002047546731773764\n",
      "#8600 Loss: 0.00020253683032933623\n",
      "#8700 Loss: 0.000200368303922005\n",
      "#8800 Loss: 0.0001982464746106416\n",
      "#8900 Loss: 0.0001961727684829384\n",
      "#9000 Loss: 0.00019414463895373046\n",
      "#9100 Loss: 0.00019215878273826092\n",
      "#9200 Loss: 0.00019021551997866482\n",
      "#9300 Loss: 0.00018831367196980864\n",
      "#9400 Loss: 0.0001864501682575792\n",
      "#9500 Loss: 0.00018462419393472373\n",
      "#9600 Loss: 0.0001828371168812737\n",
      "#9700 Loss: 0.000181084600626491\n",
      "#9800 Loss: 0.00017936769290827215\n",
      "#9900 Loss: 0.00017768632096704096\n",
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "tensor([0.5000, 1.0000], device='privateuseone:0')\n",
      "Output: \n",
      "tensor([0.9493], device='privateuseone:0')\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "print(\"Starting training...\")\n",
    "for i in range(10000):  # trains the NN 10,000 times\n",
    "    if (i % 100) == 0:\n",
    "        print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
    "    #Confirm everything is running on DML not CPU\n",
    "    try:\n",
    "        NN.train(X, y)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Error: Training failed\")\n",
    "        break\n",
    "NN.saveWeights(NN)\n",
    "NN.predict()\n",
    "\n",
    "print(\"Finished training!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T20:21:28.095750Z",
     "end_time": "2023-04-20T20:21:33.397016Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
